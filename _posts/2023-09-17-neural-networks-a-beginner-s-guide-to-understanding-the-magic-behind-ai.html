---
layout: post
title: 'Neural Networks: A Beginnerâ€™s Guide to Understanding the Magic Behind AI'
tag:
- ai
- neural-networks
- computer-science
---

<p>Welcome traveler, in the growing world of intelligence there is a term that always stands out as incredibly influential, â€œneural networks.â€ These computerized structures, which draw inspiration from the mechanisms of the brain serve as the foundation, for state of the art technologies that drive various applications, like autonomous vehicles and voice recognition systems.</p><p>After a short break, Iâ€™m back with another interesting article, where we look deep into how neural networks work. I will try to explain it in a simple way and hopefully, you will find out that in the end itâ€™s not thatÂ hard.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KEJjZa_Z27qWZVhr6S-Jyw.jpeg" /></figure><p>Iâ€™m not a professional and I could be wrong if you spot any mistakes, head into commentsÂ ğŸ™‚.</p><p>With that said letâ€™s jump into the darknessâ€¦</p><h3>You willÂ learn</h3><h4><strong>Fundamentals of NeuralÂ Networks</strong></h4><ul><li>What Are Neural Networks?</li><li>Neurons</li></ul><h4><strong>Neural NetworkÂ Layers</strong></h4><ul><li>Input Layer</li><li>Hidden Layers</li><li>Output Layer</li></ul><h4><strong>Common Implementations</strong></h4><ul><li>Feedforward Neural NetworksÂ (FNN)</li><li>Convolutional Neural NetworksÂ (CNN)</li><li>Recurrent Neural NetworksÂ (RNN)</li><li>Long Short-Term Memory NetworksÂ (LSTM)</li></ul><h3>What Are Neural Networks?</h3><p>You could think of them like equivalent of the human brainâ€™s information processing system. They are designed to mimic the brainâ€™s ability to learn, adapt, and process data. These networks consist of artificial neurons, or nodes, interconnected by weighted connections, the whole calls it self: Artificial Neural NetworkÂ (ANN).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*da54QdIzKOetYf98" /><figcaption>Visual representation of â€œneural networkâ€.</figcaption></figure><p>Neural networks are inspired by the workings of the brain. In our brains neurons communicate by transmitting signals through synapses. Artificial neural networks replicate this process usingÂ models.</p><p>These networks are exceptional, at recognizing patterns making predictions and handling volumes of data, which is crucial for tasks such, as image recognition, language comprehension and recommendation systems.</p><h3>Neurons</h3><blockquote>Building Blocks of NeuralÂ Networks</blockquote><p>In a neural network, each neuron acts as a processing unit.</p><p>Artificial neurons take input data, perform calculations on it, and produce anÂ output.</p><p>These outputs are then used as inputs for subsequent layers or are the final predictions of theÂ network.</p><h4><strong>Weights andÂ Biases</strong></h4><p>Two essential components are weights and biases. These elements determine how strongly an input signal influences the neuronâ€™sÂ output.</p><ul><li><strong>Weights</strong>ğŸ’ª<strong>:</strong> Weights is the strength of connections between neurons. Each connection between neurons has an associated weight. Larger weights amplify the inputâ€™s impact on the neuronâ€™s output, while smaller weights diminishÂ it.</li><li><strong>Biases</strong>ğŸ‘¤<strong>:</strong> Biases are like an offset or threshold for the neuron. They are shifting the activation function left or right. If you want to read more about biases you can clickÂ <a href="https://www.turing.com/kb/necessity-of-bias-in-neural-networks">here</a>.</li></ul><p>Weights and biases enable neurons to learn and adjust their behavior during training. The process of determining the appropriate weights and biases, often known as model training or optimization.</p><p>They are capable of learning complicated patterns and correlations among data, making them useful for tasks like as image recognition, natural language interpretation, and manyÂ others.</p><h4>Example of neural prediction</h4><p>Imagine you are training a neural network to predict whether a student will pass a test based on the number of hours they study. Your input data consists of the number of hours a student studied, and the output is representing whether they passed (1) or failedÂ (0).</p><p>To make this prediction, the neuron performs the following operation:</p><blockquote>Output = Activation Function(Weight Ã— Number of Hours Studied +Â Bias)</blockquote><ul><li><strong>Activation function</strong>âš¡ï¸: It determines whether a neuron should â€œfireâ€ (output a signal) or not based on the input it receives. The most frequent functions are <strong>Sigmoid</strong>, <strong>ReLU </strong>(Rectified Linear Unit), <strong>Tanh</strong> (hyperbolic tangent)<strong>.</strong></li></ul><p>Here the bias can represent how hard is the test, Letâ€™s say the weight is 0.1, and the bias is -5 and you studied forÂ 40hours.</p><blockquote>Output = Activation Function(0.1 x 40 x -5) =Â -1</blockquote><p>Since itâ€™s -1 meaning you wouldnâ€™t pass. Of course predicting the test like this is nonce, but you got theÂ point.</p><h3><strong>Neural NetworkÂ Layers</strong></h3><p>Neural networks are made up of layers, each having its own function and characteristics.</p><p>Understanding these levels is critical for understanding how information moves through a network and how the decisions areÂ made.</p><p>Weâ€™ll look at the three main types of layers in a neural network architecture: the Input Layer, Hidden Layers, and OutputÂ Layer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/538/1*REzVHUebWj8lFpsqWSnpdw.png" /><figcaption>Network Layers</figcaption></figure><h4><strong>Input Layer</strong></h4><blockquote>The Entry Point ofÂ Data</blockquote><p>The first layer of a neural network is the Input Layer. Its major duty is to receive data and forward it to the following layers for processing.</p><p>Each neuron in the input layer corresponds to a feature or input variable, it serves as an important link between the external data and the neuralÂ network.</p><ul><li><strong>No Computation</strong>ğŸš§<strong>:</strong> Neurons in the input layer donâ€™t perform any computation. They simply transmit the input data to the hiddenÂ layers.</li><li><strong>Size Determination</strong>ğŸ”<strong>:</strong> The number of neurons in the input layer is determined by the dimensionality of the input data. For instance, in an image classification task, each pixel might correspond to a neuron in the inputÂ layer.</li></ul><p>You may encounter situations where input values are constrained to a specific range [0, 1] thatâ€™s called standardization and normalization. It can help the neural network converge faster during training and make it more robust to different input scales. However, these techniques are not always necessary and depend on the type ofÂ data.</p><h4><strong>Hidden Layers</strong></h4><blockquote>The Workhorses of theÂ Network</blockquote><p>In a neural network, the true computing takes place in the Hidden Layers. These layers are in charge of extracting characteristics from input data as well as learning complex patterns and representations.</p><p>While a network can have numerous hidden layers, the number and size of hidden layers are determined by the networkâ€™s architecture and the taskâ€™s complexity.</p><ul><li><strong>Weighted Sum</strong>â•: The values provided are multiplied by their weights. These weighted sums are computed by taking the dot product, between the input values and the corresponding neuron weights. The outcome is a collection ofÂ sums.</li><li><strong>Bias Addition</strong>â•: After calculating the weighted sums, each neuron adds a biasÂ term.</li><li><strong>Activation Function</strong>âš¡ï¸: The result of the weighted sum plus the bias for each neuron is then passed through an activation function.</li><li><strong>Hierarchical Learning</strong>ğŸ§ <strong>:</strong> Hidden layers progressively learn more abstract and high-level features as information flows through the networkâ€™s layers.</li></ul><p>The hidden layers of a neural network act as feature extractors.</p><h4><strong>Output Layer</strong></h4><blockquote>Making Predictions</blockquote><p>Final layer of the neural network. It produces the networkâ€™s predictions or outputs based on the processed information from the hidden layers. The structure and number of neurons in the output layer depend on the nature of theÂ task.</p><ul><li><strong>Loss Calculation</strong>ğŸ“‰: Once the network produces its output, the loss or error between the predicted values and the actual target values (ground truth) is calculated. This loss is a measure of how well the network is performing.</li><li><strong>Backpropagation and Training</strong>ğŸ”„: The computed loss is used to update the networkâ€™s weights and biases through a process called backpropagation and optimization algorithms like gradient descent. The network learns to adjust its parameters to minimize the loss, making its predictions more accurate overÂ time.</li><li><strong>Interpretation</strong>ğŸ“–<strong>:</strong> The values produced by the neurons in the output layer are often interpreted as probabilities or class scores, depending on the problem. The highest-scoring class or classes are the networkâ€™s final predictions.</li></ul><p>The final predictions or values are created in the output layer based on the networkâ€™s learnt weights and biases. Networkâ€™s performance is enhanced by training, in which it learns to make better predictions by altering its parameters.</p><h3><strong>Common Implementations</strong></h3><p>Neural networks come in a variety of shapes and sizes, each one customized to specific types of data and activities. Iâ€™ll introduce you to four common neural network implementations and share insights on where theyÂ thrive.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vTsDqjgjM1ROqYW9ITIgDw.jpeg" /></figure><h4><strong>Feedforward Neural NetworksÂ (FNN)</strong></h4><blockquote>The Foundation of NeuralÂ Networks</blockquote><p>Feedforward Neural Networks, often referred to as multilayer perceptrons (MLPs), serve as the foundational architecture for neural networks. They are versatile and can be used for a wide range of tasks, including classification and regression.</p><ul><li><strong>Architecture</strong>ğŸ°<strong>:</strong> FNNs consist of an input layer, one or more hidden layers, and an output layer. Information flows in one direction, from input to output, without loops orÂ cycles.</li><li><strong>Applications</strong>ğŸ“‹<strong>:</strong> FNNs find applications in image classification, sentiment analysis, and other structured data tasks. Their simplicity and effectiveness make them a valuable choice for many problems.</li></ul><h4><strong>Convolutional Neural NetworksÂ (CNN)</strong></h4><blockquote>Unraveling Image and SpatialÂ Data</blockquote><p>Convolutional Neural Networks are specifically designed for tasks involving grid-like data, such as images and spatial data. They excel at capturing spatial hierarchies and patterns.</p><ul><li><strong>Architecture</strong>ğŸ°<strong>:</strong> CNNs incorporate convolutional layers for feature extraction and pooling layers for dimensionality reduction. These layers allow the network to understand spatial relationships in theÂ data.</li><li><strong>Applications</strong>ğŸ“‹<strong>:</strong> CNNs are the go-to choice for image classification, object detection, facial recognition, and tasks involving grid-like data.</li></ul><h4><strong>Recurrent Neural NetworksÂ (RNN)</strong></h4><blockquote>Mastering Sequential Data</blockquote><p>Recurrent Neural Networks are designed for tasks that involve sequential data, where the order of elements matters. They have memory cells that enable them to retain information from previous timeÂ steps.</p><ul><li><strong>Architecture</strong>ğŸ°<strong>:</strong> RNNs have recurrent connections that loop back on themselves, allowing them to maintain a form of memory. This makes them suitable for tasks like natural language processing, time series prediction, and speech recognition.</li><li><strong>Applications</strong>ğŸ“‹<strong>:</strong> RNNs are used in machine translation, text generation, speech synthesis, and any task involving sequences.</li></ul><h4><strong>Long Short-Term Memory NetworksÂ (LSTM)</strong></h4><blockquote>Overcoming the Shortcomings ofÂ RNNs</blockquote><p>While RNNs are powerful for sequential data, they suffer from vanishing gradient problems. Long Short-Term Memory Networks (LSTMs) were developed to address these issues by introducing specialized memoryÂ cells.</p><ul><li><strong>Architecture</strong>ğŸ°<strong>:</strong> LSTMs include memory cells with gating mechanisms that control the flow of information, allowing them to capture long-range dependencies in sequential data.</li><li><strong>Applications</strong>ğŸ“‹<strong>:</strong> LSTMs are widely used in tasks requiring long-term dependencies, such as speech recognition, language modeling, and sentiment analysis.</li></ul><p>These are just a few of the common neural network architectures, each tailored to different data types and tasks. As we explore these implementations further, youâ€™ll gain a deeper understanding of how to choose the right architecture for your specific AI and machine learning projects.</p><h3>Conclusion</h3><p>They can be used almost everywhere and they are ground breaking because they can remove the repetitive tasks from daily life, but at their core they cannot replace humans.. So, no worriesÂ ğŸ˜Š.</p><p>Anyway, If you enjoyed this article, clapğŸ‘‹ and follow meğŸ“‘! Thanks for reading! Looking forward to seeing you in theÂ future.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=153bae652a2d" width="1" height="1" alt="">
